'''
Updated on May 11, 2023

@author: Simona Bernardi, Guillermo CÃ¡novas

Preprocessing of the Ausgrid dataset:
-- it generates a set of files with the same format as the ISSDA-CER that is:
ID,DT,Usage
where:
- ID meter identifier
- DT time stamp NNNHH, where  NNN day number and  HH in [01,48] half-an-hour register
- Usage that is actually the electricity produced 
-- all the meterIDs have a complete dataset 
-- Just the datasets 2010-2011 Solar home electricity data.csv and 2011-2012 Solar home electricity data v2.csv
   have been considered. The last one 2012-2013SolarHomeElectricityDatav2.csv includes not complete readings
   for some meterID.
'''

import sys
import os
import csv
import pandas as pd
import numpy as np
from datetime import datetime
import re
import itertools


#DATASET PATHS
DATASET_PATH = "./datasets/Ausgrid/data"
DATASET_PREPROCESSED_PATH = "./datasets/Ausgrid/data_all_filtered"

#Constants of the specific datasets
OBS_IN_DAY = 48 # readings in a day

def convert_datetime_to_dt(date):
  # Regular expresions to verify if the format is dd/mm/yyyy or d-mmm-yy
  regex_first_format = r'^\d{1,2}/\d{1,2}/\d{2,4}$'
  regex_second_format = r'^\d{1,2}-[a-zA-Z]{3}-\d{2}$'

  if re.match(regex_first_format, date):
    fecha = datetime.strptime(date, '%d/%m/%Y')
  elif re.match(regex_second_format, date):
    fecha = datetime.strptime(date, '%d-%b-%y')

  day_of_the_year = fecha.timetuple().tm_yday
  return day_of_the_year


def missing_rows(file):
  df_orig = pd.read_csv(file, skiprows=1) # Lee el DataFrame original
  cols = ['Customer','date']
  df_orig = df_orig[df_orig['Consumption Category'] == 'CL']
  df_orig = df_orig[cols]

  ids = df_orig['Customer'].drop_duplicates() # Genera todas las combinaciones posibles de ID y Date

  dates = pd.date_range(start='07/01/2011', end='06/30/2012')
  dates_str = dates.strftime('%-d/%m/%Y').tolist()

  combinations = list(itertools.product(ids, dates_str))

  df_all = pd.DataFrame(combinations, columns=['Customer', 'date']) # Crea un nuevo DataFrame con las combinaciones

  # Encuentra las combinaciones que faltan
  df_missing = pd.merge(df_all, df_orig, on=['Customer', 'date'], how='outer', indicator=True)
  df_missing = df_missing[df_missing['_merge'] == 'left_only'].drop('_merge', 1)

  half_hour_columns = pd.date_range(start='0:00', end='23:30', freq='30min').strftime('%-H:%M').tolist()
  half_hour_columns.append(half_hour_columns.pop(0))

  new_df = pd.DataFrame(0, index=df_missing.index, columns=half_hour_columns)
  df_missing = pd.concat([df_missing, new_df], axis=1)

  return df_missing


class dataAnalyzer:

    def __init__(self):
        return None;
 
    def loadReorganizeFiles(self, files, type):
        firstDayCode = 0
        for file in files:
            print("Uploading file:", file)
            
            # Skip the first rows (just comments)
            origin = DATASET_PATH + "/" + file
            dset = pd.read_csv(origin, skiprows=1)

            #Filter the cols of interest
            cols = ['Customer','date','0:30','1:00','1:30','2:00','2:30','3:00','3:30','4:00','4:30',
                '5:00','5:30','6:00','6:30','7:00','7:30','8:00','8:30','9:00','9:30',
                '10:00','10:30','11:00','11:30','12:00','12:30','13:00','13:30','14:00',
                '14:30','15:00','15:30','16:00','16:30','17:00','17:30','18:00','18:30',
                '19:00','19:30','20:00','20:30','21:00','21:30','22:00','22:30','23:00',
                '23:30','0:00']

            if(type == 'generation'):
                # Filter the rows with electricity generation data
                # GG = Gross Generation for electricity generated by the solar system with a gross 
                # metering configuration, measured separately to household loads
                dset = dset[dset['Consumption Category'] == 'GG']
                dset = dset[cols]

            elif (type == 'consumption'):
                # Filter the rows with electricity consumpton data
                # GC = General Consumption for electricity supplied all the time excluding solar generation and controlled load supply
                # CL = Controlled Load Consumption
                dset_aux = dset[dset['Consumption Category'] == 'CL']
                dset = dset[dset['Consumption Category'] == 'GC']

                dset = dset[cols]
                dset_aux = dset_aux[cols]

                #Get number of different dates and create a date number array
                dates_aux = dset_aux['date'].drop_duplicates()
                dn_aux = np.empty(len(dates_aux), dtype= int)

                first_date_aux = dates_aux.iloc[0]
                first_date_num_aux = convert_datetime_to_dt(first_date_aux)
                
                idx = firstDayCode
                for i in range(len(dates_aux)):
                    dn_aux[i] = first_date_num_aux + idx
                    idx+=1
                
                #Get half-an-hours
                hh=["%02d" % (x+1) for x in range(OBS_IN_DAY)]
                dt_aux = np.empty(len(dates_aux)*OBS_IN_DAY)
                i = 0
                for d in dn_aux:
                    for h in hh:
                        dt_aux[i] = str(d) + h
                        i += 1

                #Get the meterIDs
                meterID = dset_aux['Customer'].drop_duplicates()

                #Set the DT array
                dt_aux = np.tile(dt_aux,len(meterID)) 
                dt_aux = dt_aux.astype(int)
                        
                #Set meterID array
                ID_aux = []
                for i in meterID:
                    ID_aux = np.concatenate( (ID_aux,np.tile(i,len(dates_aux)*OBS_IN_DAY)), axis= None)
                ID_aux = ID_aux.astype(int)
            
            #Get number of different dates and create a date number array
            dates = dset['date'].drop_duplicates()
            dn = np.empty(len(dates), dtype= int)

            first_date = dates.iloc[0]
            first_date_num = convert_datetime_to_dt(first_date)
            
            idx = firstDayCode
            for i in range(len(dates)):
                dn[i] = first_date_num + idx
                idx+=1

            #Get half-an-hours
            hh=["%02d" % (x+1) for x in range(OBS_IN_DAY)]
            dt = np.empty(len(dates)*OBS_IN_DAY)
            i = 0
            for d in dn:
                for h in hh:
                    dt[i] = str(d) + h
                    i += 1

            #Get the meterIDs
            meterID = dset['Customer'].drop_duplicates()

            #Set the DT array
            dt = np.tile(dt,len(meterID)) 
            dt = dt.astype(int)
            
            #Set meterID array
            ID = []
            for i in meterID:
                ID = np.concatenate( (ID,np.tile(i,len(dates)*OBS_IN_DAY)), axis= None)
            ID = ID.astype(int)

            if type == 'generation':
                #Set the production data array
                dset = dset[cols[2:]]
                dset = dset.to_numpy().flatten()

            elif type == 'consumption':
                #Set the production data array
                dset = dset[cols[2:]]
                dset = dset.to_numpy().flatten()

                #Missing rows have now a Usage value of 0 in each half hour
                if (len(ID_aux) != len(dt_aux)) or (len(dt_aux) != len((dset_aux[cols[2:]]).to_numpy().flatten())):
                    print("There are empty rows in the dataset...")
                    print("We are going to create those rows with the Usage value equal to 0 in each half hour!")

                    df_missing_rows = missing_rows(origin)  
                    df_full_rows = pd.concat([dset_aux, df_missing_rows], axis=0)
                    df_full_rows = df_full_rows.sort_values(['Customer', 'date'])

                    dset_aux = df_full_rows[cols[2:]]
                    dset_aux = dset_aux.to_numpy().flatten()

                else:
                    dset_aux = dset_aux[cols[2:]]
                    dset_aux = dset_aux.to_numpy().flatten()
                
                #Reorganize data
                df = pd.DataFrame({'ID': ID, 'DT': dt, 'Usage': dset})
                df_aux = pd.DataFrame({'ID': ID_aux, 'DT': dt_aux, 'Usage': dset_aux})

                merged_df = pd.merge(df, df_aux, on=['ID', 'DT'], how='left', suffixes=('_x', '_y')) # Merge both Dataframes using 'ID' and 'DT' columns as the key
                merged_df['Usage'] = merged_df['Usage_x'].fillna(0) + merged_df['Usage_y'].fillna(0) # Sum columns 'Usage_x' and 'Usage_y' in a new column call 'Usage'
                df = merged_df[['ID', 'DT', 'Usage']] # Select the columns 'ID', 'DT' and 'Usage' from the resulting DataFrame

            # Store... Generate  file
            dest = DATASET_PREPROCESSED_PATH + '/' + type + '/' + file
            df.to_csv(dest, index=False, header=True, float_format='%.2f')  # 2 decimal points (even ".00")

            firstDayCode = len(dates)

def preprocessing(type):

    # new dataAnalyzer object
    mg = dataAnalyzer()

    dirFiles = os.listdir(DATASET_PATH)
    # Sort the files (just in case)
    dirFiles = sorted(dirFiles)
 
    mg.loadReorganizeFiles(dirFiles[0:2], type.lower())
 
 
def check_preconditions():
    # Checking invocation parameters
    if len(sys.argv) != 2:
        print("Usage (from the diaspore directory): python3 ausgrid.py <consumption/generation>")
        exit(85)

    # Preventing execution error
    data_directories_not_exists = not os.path.isdir(DATASET_PATH) or not os.path.isdir(DATASET_PREPROCESSED_PATH)
 
    if data_directories_not_exists:
        print("The following directories are needed:")
        print(DATASET_PATH)
        print(DATASET_PREPROCESSED_PATH)
        exit(1)


    # Preventing bad behaviour with appending mode
    data_is_not_empty = len(os.listdir(DATASET_PREPROCESSED_PATH + '/' + sys.argv[1])) > 0
    if data_is_not_empty:
        print("Remove all files of ",DATASET_PREPROCESSED_PATH," before executing this script")
        exit(1)



if __name__ == '__main__':
    """
    args: type = CONSUMPTION or GENERATION
    """

    check_preconditions()
    print("Preconditions OK, starting with the process......\n")

    preprocessing(sys.argv[1])
    print("Preprocessing done. Preprocessed dataset in: ", DATASET_PREPROCESSED_PATH, "\n")
